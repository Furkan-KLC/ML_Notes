{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab5140b",
   "metadata": {},
   "source": [
    "# CLUSTERING ALGORITHMS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b9498",
   "metadata": {},
   "source": [
    "# Complete Guide to Clustering Algorithms for ML Engineers\n",
    "\n",
    "## 1. Partition-Based Clustering\n",
    "\n",
    "### K-Means\n",
    "\n",
    "**How it works:**\n",
    "K-Means partitions data into k clusters by minimizing within-cluster sum of squares (WCSS). Uses centroids to represent clusters.\n",
    "\n",
    "**Core Algorithm:**\n",
    "1. **Initialize**: Place k centroids randomly in feature space\n",
    "2. **Assign**: Assign each point to nearest centroid (Euclidean distance)\n",
    "3. **Update**: Move centroids to mean of assigned points\n",
    "4. **Iterate**: Repeat assign-update until convergence\n",
    "5. **Convergence**: Stop when centroids stop moving or max iterations reached\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "- Objective: Minimize WCSS = Σᵢ Σₓ∈Cᵢ ||x - μᵢ||²\n",
    "- Uses Lloyd's algorithm for optimization\n",
    "- NP-hard problem, algorithm finds local minimum\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and fast\n",
    "- Works well with spherical clusters\n",
    "- Guaranteed convergence\n",
    "- Memory efficient\n",
    "- Good for large datasets\n",
    "\n",
    "**Disadvantages:**\n",
    "- Need to specify k beforehand\n",
    "- Sensitive to initialization\n",
    "- Assumes spherical clusters\n",
    "- Sensitive to outliers\n",
    "- Struggles with varying cluster sizes\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Customer segmentation\n",
    "- Image segmentation\n",
    "- Market research\n",
    "- Data compression\n",
    "- Feature learning\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=8,           # Number of clusters\n",
    "    init='k-means++',       # Initialization method\n",
    "    n_init=10,             # Number of random initializations\n",
    "    max_iter=300,          # Maximum iterations\n",
    "    tol=1e-4,              # Tolerance for convergence\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### MiniBatchKMeans\n",
    "\n",
    "**How it works:**\n",
    "Variant of K-Means that uses mini-batches of data for faster computation on large datasets.\n",
    "\n",
    "**Core Differences from K-Means:**\n",
    "1. **Mini-batch Processing**: Uses random subsets of data in each iteration\n",
    "2. **Faster Updates**: Updates centroids using mini-batch instead of full dataset\n",
    "3. **Memory Efficient**: Processes data in smaller chunks\n",
    "4. **Trade-off**: Slightly lower quality for much faster speed\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. Initialize centroids like K-Means\n",
    "2. Sample mini-batch from dataset\n",
    "3. Assign points in mini-batch to nearest centroids\n",
    "4. Update centroids using learning rate\n",
    "5. Repeat with new mini-batch until convergence\n",
    "\n",
    "**Advantages:**\n",
    "- Much faster than standard K-Means\n",
    "- Lower memory usage\n",
    "- Can handle streaming data\n",
    "- Good for very large datasets\n",
    "- Similar results to K-Means\n",
    "\n",
    "**Disadvantages:**\n",
    "- Slightly lower cluster quality\n",
    "- Additional hyperparameter (batch_size)\n",
    "- May need more iterations\n",
    "- Less stable than K-Means\n",
    "\n",
    "**When to Use:**\n",
    "- Datasets with millions of samples\n",
    "- Limited computational resources\n",
    "- Online/streaming clustering\n",
    "- When speed is more important than perfect accuracy\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "mini_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=8,\n",
    "    batch_size=100,        # Size of mini-batches\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "## 2. Density-Based Clustering\n",
    "\n",
    "### DBSCAN (Density-Based Spatial Clustering)\n",
    "\n",
    "**How it works:**\n",
    "Groups together points in high-density areas and marks points in low-density areas as outliers.\n",
    "\n",
    "**Core Concepts:**\n",
    "1. **ε (epsilon)**: Maximum distance between points to be neighbors\n",
    "2. **MinPts**: Minimum points required to form dense region\n",
    "3. **Core Points**: Points with ≥ MinPts neighbors within ε\n",
    "4. **Border Points**: Non-core points within ε of core points\n",
    "5. **Noise Points**: Points that are neither core nor border\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. For each unvisited point:\n",
    "2. Find all neighbors within ε distance\n",
    "3. If neighbors ≥ MinPts, start new cluster with this core point\n",
    "4. Add all density-reachable points to cluster\n",
    "5. Mark isolated points as noise\n",
    "6. Repeat until all points processed\n",
    "\n",
    "**Advantages:**\n",
    "- Finds clusters of arbitrary shape\n",
    "- Automatically determines number of clusters\n",
    "- Robust to outliers\n",
    "- Can find noise points\n",
    "- No assumption about cluster shape\n",
    "\n",
    "**Disadvantages:**\n",
    "- Sensitive to hyperparameters (ε, MinPts)\n",
    "- Struggles with varying densities\n",
    "- High-dimensional data issues\n",
    "- Can be slow on large datasets\n",
    "- Difficult parameter tuning\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Anomaly detection\n",
    "- Image processing\n",
    "- Social network analysis\n",
    "- Geolocation clustering\n",
    "- When cluster shapes are irregular\n",
    "\n",
    "**Parameter Selection Tips:**\n",
    "- **ε**: Use k-distance graph, look for \"elbow\"\n",
    "- **MinPts**: Rule of thumb: 2 × dimensions\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(\n",
    "    eps=0.5,               # Neighborhood distance\n",
    "    min_samples=5,         # Minimum points for core point\n",
    "    metric='euclidean',    # Distance metric\n",
    "    algorithm='auto'       # Algorithm for neighbor search\n",
    ")\n",
    "```\n",
    "\n",
    "### OPTICS (Ordering Points To Identify Clustering Structure)\n",
    "\n",
    "**How it works:**\n",
    "Extension of DBSCAN that addresses the problem of varying densities by creating an ordering of points.\n",
    "\n",
    "**Core Concepts:**\n",
    "1. **Core Distance**: Minimum ε needed for point to be core\n",
    "2. **Reachability Distance**: Distance between points considering density\n",
    "3. **Ordering**: Linear ordering of points showing cluster structure\n",
    "4. **Reachability Plot**: Visualization showing cluster hierarchy\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. Calculate core distances for all points\n",
    "2. Process points in specific order\n",
    "3. Update reachability distances\n",
    "4. Create ordering that reveals cluster structure\n",
    "5. Extract clusters using different ε values from ordering\n",
    "\n",
    "**Advantages:**\n",
    "- Handles varying cluster densities\n",
    "- Provides cluster hierarchy\n",
    "- More robust parameter selection\n",
    "- Can find nested clusters\n",
    "- Produces reachability plot for analysis\n",
    "\n",
    "**Disadvantages:**\n",
    "- More complex than DBSCAN\n",
    "- Requires additional post-processing\n",
    "- Higher computational complexity\n",
    "- Still sensitive to MinPts parameter\n",
    "- Harder to interpret\n",
    "\n",
    "**When to Use:**\n",
    "- Data with varying densities\n",
    "- Need hierarchical cluster structure\n",
    "- Want to explore different density levels\n",
    "- DBSCAN results are unsatisfactory\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "optics = OPTICS(\n",
    "    min_samples=5,         # Minimum points for core point\n",
    "    max_eps=np.inf,        # Maximum ε value\n",
    "    metric='euclidean',\n",
    "    cluster_method='xi'    # Method for extracting clusters\n",
    ")\n",
    "```\n",
    "\n",
    "### MeanShift\n",
    "\n",
    "**How it works:**\n",
    "Density-based algorithm that finds modes (peaks) in the probability density function by shifting points toward higher density regions.\n",
    "\n",
    "**Core Concepts:**\n",
    "1. **Kernel Density Estimation**: Uses kernel functions to estimate density\n",
    "2. **Mean Shift Vector**: Direction toward higher density\n",
    "3. **Bandwidth**: Controls the size of the kernel\n",
    "4. **Mode Seeking**: Each point moves toward local density maximum\n",
    "5. **Convergence**: Points converge to density modes (cluster centers)\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. Initialize each point as potential cluster center\n",
    "2. For each point, calculate mean shift vector\n",
    "3. Move point in direction of mean shift\n",
    "4. Repeat until convergence\n",
    "5. Points converging to same mode form cluster\n",
    "6. Remove duplicate cluster centers\n",
    "\n",
    "**Advantages:**\n",
    "- Automatically finds number of clusters\n",
    "- No assumption about cluster shape\n",
    "- Can find arbitrary-shaped clusters\n",
    "- Robust to outliers\n",
    "- Single parameter (bandwidth)\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive O(n²)\n",
    "- Sensitive to bandwidth parameter\n",
    "- Can be slow on large datasets\n",
    "- May not work well in high dimensions\n",
    "- Bandwidth selection is critical\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Image segmentation\n",
    "- Object tracking\n",
    "- Peak finding in data\n",
    "- When number of clusters is unknown\n",
    "- Computer vision applications\n",
    "\n",
    "**Bandwidth Selection:**\n",
    "- Too small: Over-segmentation\n",
    "- Too large: Under-segmentation\n",
    "- Use cross-validation or estimate_bandwidth()\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "\n",
    "# Estimate bandwidth\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2)\n",
    "\n",
    "meanshift = MeanShift(\n",
    "    bandwidth=bandwidth,    # Kernel bandwidth\n",
    "    seeds=None,            # Initial centers\n",
    "    bin_seeding=False      # Speed optimization\n",
    ")\n",
    "```\n",
    "\n",
    "## 3. Hierarchical Clustering\n",
    "\n",
    "### AgglomerativeClustering\n",
    "\n",
    "**How it works:**\n",
    "Bottom-up hierarchical clustering that starts with individual points as clusters and successively merges closest clusters.\n",
    "\n",
    "**Core Concepts:**\n",
    "1. **Linkage Criteria**: Method to measure distance between clusters\n",
    "2. **Dendrogram**: Tree showing merge hierarchy\n",
    "3. **Distance Matrix**: Pairwise distances between all points\n",
    "4. **Merge Strategy**: How to combine clusters at each step\n",
    "\n",
    "**Linkage Methods:**\n",
    "- **Single**: Minimum distance between any two points\n",
    "- **Complete**: Maximum distance between any two points  \n",
    "- **Average**: Average distance between all pairs\n",
    "- **Ward**: Minimizes within-cluster variance (most popular)\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. Start with n clusters (each point is a cluster)\n",
    "2. Calculate distance matrix between all clusters\n",
    "3. Find pair of closest clusters\n",
    "4. Merge closest clusters\n",
    "5. Update distance matrix\n",
    "6. Repeat until desired number of clusters or single cluster\n",
    "\n",
    "**Advantages:**\n",
    "- Creates cluster hierarchy\n",
    "- No need to specify number of clusters beforehand\n",
    "- Deterministic results\n",
    "- Can capture nested cluster structures\n",
    "- Works with any distance metric\n",
    "\n",
    "**Disadvantages:**\n",
    "- O(n³) time complexity\n",
    "- Sensitive to outliers (single linkage)\n",
    "- Cannot undo previous steps\n",
    "- Memory intensive for large datasets\n",
    "- May create unbalanced clusters\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Phylogenetic analysis\n",
    "- Social network analysis\n",
    "- Market segmentation with hierarchy\n",
    "- Image segmentation\n",
    "- When cluster hierarchy is important\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(\n",
    "    n_clusters=2,          # Number of clusters\n",
    "    affinity='euclidean',  # Distance metric\n",
    "    linkage='ward',        # Linkage criterion\n",
    "    distance_threshold=None # Alternative to n_clusters\n",
    ")\n",
    "```\n",
    "\n",
    "### FeatureAgglomeration\n",
    "\n",
    "**How it works:**\n",
    "Hierarchical clustering applied to features instead of samples, used for dimensionality reduction.\n",
    "\n",
    "**Core Purpose:**\n",
    "- Groups similar features together\n",
    "- Reduces feature dimensionality\n",
    "- Creates feature clusters instead of sample clusters\n",
    "- Used as preprocessing step\n",
    "\n",
    "**Algorithm:**\n",
    "1. Transpose data matrix (features become samples)\n",
    "2. Apply agglomerative clustering to features\n",
    "3. Each cluster represents similar features\n",
    "4. Can average features within clusters for dimensionality reduction\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces feature space\n",
    "- Maintains interpretability\n",
    "- Can improve model performance\n",
    "- Removes redundant features\n",
    "- Hierarchical feature relationships\n",
    "\n",
    "**Disadvantages:**\n",
    "- May lose important information\n",
    "- Requires domain knowledge for interpretation\n",
    "- Computational overhead\n",
    "- Not suitable for all feature types\n",
    "\n",
    "**Use Cases:**\n",
    "- Dimensionality reduction\n",
    "- Feature selection\n",
    "- Gene expression analysis\n",
    "- Text mining\n",
    "- When features are highly correlated\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "feature_agg = FeatureAgglomeration(\n",
    "    n_clusters=50,         # Number of feature clusters\n",
    "    affinity='euclidean',\n",
    "    linkage='ward'\n",
    ")\n",
    "```\n",
    "\n",
    "## 4. Gaussian Mixture Models\n",
    "\n",
    "### GaussianMixture\n",
    "\n",
    "**How it works:**\n",
    "Probabilistic model assuming data comes from a mixture of Gaussian distributions with unknown parameters.\n",
    "\n",
    "**Core Concepts:**\n",
    "1. **Mixture Components**: Each cluster is a Gaussian distribution\n",
    "2. **EM Algorithm**: Expectation-Maximization for parameter estimation\n",
    "3. **Soft Clustering**: Points have probabilistic membership\n",
    "4. **Parameters**: Mean, covariance, and mixing weights for each component\n",
    "5. **Maximum Likelihood**: Finds parameters that maximize data likelihood\n",
    "\n",
    "**Algorithm Steps (EM):**\n",
    "1. **Initialization**: Initialize parameters for k Gaussians\n",
    "2. **E-step**: Calculate probability of each point belonging to each component\n",
    "3. **M-step**: Update parameters based on weighted point assignments\n",
    "4. **Convergence**: Repeat E-M steps until parameters stabilize\n",
    "5. **Assignment**: Assign points to component with highest probability\n",
    "\n",
    "**Covariance Types:**\n",
    "- **'full'**: Each component has own covariance matrix\n",
    "- **'tied'**: All components share same covariance matrix\n",
    "- **'diag'**: Diagonal covariance matrices\n",
    "- **'spherical'**: Spherical covariance matrices\n",
    "\n",
    "**Advantages:**\n",
    "- Provides probabilistic cluster membership\n",
    "- Can model elliptical clusters\n",
    "- Estimates cluster parameters\n",
    "- Can generate new samples\n",
    "- Handles overlapping clusters well\n",
    "\n",
    "**Disadvantages:**\n",
    "- Need to specify number of components\n",
    "- Assumes Gaussian distributions\n",
    "- Can converge to local optima\n",
    "- Sensitive to initialization\n",
    "- May overfit with too many components\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Data with overlapping clusters\n",
    "- When probability estimates are needed\n",
    "- Density estimation\n",
    "- Generative modeling\n",
    "- Anomaly detection\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=3,        # Number of mixture components\n",
    "    covariance_type='full', # Type of covariance parameters\n",
    "    max_iter=100,          # Maximum EM iterations\n",
    "    n_init=1,              # Number of initializations\n",
    "    init_params='kmeans',  # Initialization method\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### BayesianGaussianMixture\n",
    "\n",
    "**How it works:**\n",
    "Variational Bayesian extension of Gaussian Mixture that automatically determines optimal number of components.\n",
    "\n",
    "**Key Differences from GMM:**\n",
    "1. **Bayesian Inference**: Uses prior distributions on parameters\n",
    "2. **Automatic Model Selection**: Can determine optimal number of components\n",
    "3. **Regularization**: Built-in regularization through priors\n",
    "4. **Variational Inference**: Approximates posterior distributions\n",
    "5. **Component Pruning**: Automatically removes unnecessary components\n",
    "\n",
    "**Advantages:**\n",
    "- Automatically selects number of components\n",
    "- Less prone to overfitting\n",
    "- Built-in regularization\n",
    "- More robust parameter estimation\n",
    "- Handles model uncertainty\n",
    "\n",
    "**Disadvantages:**\n",
    "- More computationally complex\n",
    "- Requires tuning of prior parameters\n",
    "- Less interpretable\n",
    "- May be conservative in component selection\n",
    "- Slower than standard GMM\n",
    "\n",
    "**When to Use:**\n",
    "- Unknown number of clusters\n",
    "- Want to avoid overfitting\n",
    "- Need uncertainty quantification\n",
    "- Small datasets\n",
    "- When model selection is critical\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "bgmm = BayesianGaussianMixture(\n",
    "    n_components=10,       # Upper bound on components\n",
    "    covariance_type='full',\n",
    "    weight_concentration_prior=1.0,\n",
    "    mean_precision_prior=1.0,\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "## 5. Spectral Clustering\n",
    "\n",
    "### SpectralClustering\n",
    "\n",
    "**How it works:**\n",
    "Uses eigenvalues and eigenvectors of data similarity matrix to perform clustering in lower-dimensional space.\n",
    "\n",
    "**Core Concepts:**\n",
    "1. **Similarity Matrix**: Measures similarity between all point pairs\n",
    "2. **Graph Laplacian**: Mathematical representation of data graph\n",
    "3. **Eigendecomposition**: Finds eigenvalues/eigenvectors of Laplacian\n",
    "4. **Embedding**: Projects data to lower-dimensional spectral space\n",
    "5. **Final Clustering**: Applies K-means in spectral space\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. Construct similarity matrix (e.g., RBF kernel)\n",
    "2. Compute graph Laplacian matrix\n",
    "3. Find k smallest eigenvectors of Laplacian\n",
    "4. Use eigenvectors as new feature representation\n",
    "5. Apply K-means clustering in spectral space\n",
    "6. Map results back to original data\n",
    "\n",
    "**Similarity Functions:**\n",
    "- **RBF (Gaussian)**: Most common, exp(-γ||x-y||²)\n",
    "- **k-nearest neighbors**: Connect k nearest neighbors\n",
    "- **ε-neighborhood**: Connect points within ε distance\n",
    "\n",
    "**Advantages:**\n",
    "- Can find non-convex clusters\n",
    "- Works well with complex cluster shapes\n",
    "- Effective for image segmentation\n",
    "- Can handle non-linearly separable data\n",
    "- Good theoretical foundation\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive (eigendecomposition)\n",
    "- Memory intensive for large datasets\n",
    "- Sensitive to similarity matrix construction\n",
    "- Need to specify number of clusters\n",
    "- Parameter tuning can be difficult\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Image segmentation\n",
    "- Social network clustering\n",
    "- Manifold clustering\n",
    "- When clusters have complex shapes\n",
    "- Computer vision applications\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "spectral = SpectralClustering(\n",
    "    n_clusters=3,          # Number of clusters\n",
    "    affinity='rbf',        # Similarity measure\n",
    "    gamma=1.0,             # Kernel parameter\n",
    "    n_neighbors=10,        # For knn affinity\n",
    "    eigen_solver='arpack', # Eigenvalue solver\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "## 6. Other Important Algorithms\n",
    "\n",
    "### BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)\n",
    "\n",
    "**How it works:**\n",
    "Hierarchical algorithm designed for large datasets using tree structure called CF-Tree.\n",
    "\n",
    "**Core Concepts:**\n",
    "1. **Clustering Feature (CF)**: Compact representation of clusters\n",
    "2. **CF-Tree**: Tree structure storing clustering features\n",
    "3. **Threshold**: Maximum diameter for subclusters\n",
    "4. **Branching Factor**: Maximum children per non-leaf node\n",
    "5. **Two-Phase**: Build CF-Tree, then apply global clustering\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. **Phase 1**: Build CF-Tree by inserting points\n",
    "2. **Phase 2**: Apply global clustering algorithm to leaf entries\n",
    "3. **Phase 3** (optional): Refine clusters\n",
    "4. **Phase 4** (optional): Redistribute points\n",
    "\n",
    "**Advantages:**\n",
    "- Handles large datasets efficiently\n",
    "- Single pass through data\n",
    "- Memory efficient\n",
    "- Can work with limited memory\n",
    "- Good for streaming data\n",
    "\n",
    "**Disadvantages:**\n",
    "- Sensitive to input order\n",
    "- Assumes spherical clusters\n",
    "- Parameter selection is critical\n",
    "- May not work well with varying densities\n",
    "- Quality depends on threshold parameter\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Very large datasets\n",
    "- Streaming data\n",
    "- Memory-constrained environments\n",
    "- Preprocessing for other algorithms\n",
    "- Real-time clustering\n",
    "\n",
    "**Key Parameters:**\n",
    "```python\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "birch = Birch(\n",
    "    n_clusters=3,          # Number of final clusters\n",
    "    threshold=0.5,         # Subcluster threshold\n",
    "    branching_factor=50,   # Maximum CF nodes\n",
    "    compute_labels=True    # Compute cluster labels\n",
    ")\n",
    "```\n",
    "\n",
    "## Additional Important Clustering Algorithms\n",
    "\n",
    "### Fuzzy C-Means (Not in sklearn, but important)\n",
    "- **Concept**: Soft clustering where points can belong to multiple clusters\n",
    "- **Use**: When clusters overlap significantly\n",
    "- **Implementation**: Available in scikit-fuzzy\n",
    "\n",
    "### HDBSCAN (Hierarchical DBSCAN)\n",
    "- **Concept**: Extends DBSCAN to varying densities using hierarchy\n",
    "- **Advantages**: Better handling of varying densities\n",
    "- **Implementation**: Available as separate package\n",
    "\n",
    "### Self-Organizing Maps (SOM)\n",
    "- **Concept**: Neural network-based clustering and visualization\n",
    "- **Use**: High-dimensional data visualization\n",
    "- **Implementation**: Available in minisom package\n",
    "\n",
    "### Affinity Propagation\n",
    "```python\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "affinity_prop = AffinityPropagation(\n",
    "    damping=0.9,           # Damping factor\n",
    "    preference=None,       # Point preferences\n",
    "    max_iter=200,\n",
    "    convergence_iter=15\n",
    ")\n",
    "```\n",
    "\n",
    "**Concept**: Finds exemplars by passing messages between data points\n",
    "**Advantages**: Automatically determines number of clusters\n",
    "**Disadvantages**: O(n²) time and space complexity\n",
    "\n",
    "## Clustering Algorithm Selection Guide\n",
    "\n",
    "| **Algorithm** | **Best For** | **Cluster Shape** | **Scalability** | **Parameters** |\n",
    "|---------------|--------------|-------------------|-----------------|----------------|\n",
    "| **K-Means** | Spherical clusters, large data | Spherical | Excellent | k (number of clusters) |\n",
    "| **MiniBatchK-Means** | Very large datasets | Spherical | Excellent | k, batch_size |\n",
    "| **DBSCAN** | Irregular shapes, noise detection | Arbitrary | Good | eps, min_samples |\n",
    "| **OPTICS** | Varying densities | Arbitrary | Moderate | min_samples |\n",
    "| **MeanShift** | Unknown k, arbitrary shapes | Arbitrary | Poor | bandwidth |\n",
    "| **Agglomerative** | Hierarchy needed | Various | Poor | n_clusters, linkage |\n",
    "| **Gaussian Mixture** | Overlapping clusters | Elliptical | Good | n_components |\n",
    "| **Spectral** | Complex manifolds | Non-convex | Poor | n_clusters, affinity |\n",
    "| **BIRCH** | Very large datasets | Spherical | Excellent | threshold, n_clusters |\n",
    "\n",
    "## Advanced Tips for ML Engineers\n",
    "\n",
    "### 1. Preprocessing Considerations\n",
    "- **Scaling**: Most algorithms require feature scaling\n",
    "- **Dimensionality**: Consider PCA/t-SNE for high dimensions\n",
    "- **Outliers**: Handle outliers before clustering (except DBSCAN)\n",
    "- **Missing Values**: Impute or use algorithms that handle them\n",
    "\n",
    "### 2. Evaluation Metrics\n",
    "- **Silhouette Score**: Measures cluster cohesion and separation\n",
    "- **Calinski-Harabasz Index**: Ratio of between-cluster to within-cluster variance\n",
    "- **Davies-Bouldin Index**: Average similarity between clusters\n",
    "- **Adjusted Rand Index**: For comparing with ground truth\n",
    "- **Inertia**: Within-cluster sum of squares (for K-means)\n",
    "\n",
    "### 3. Model Selection Strategies\n",
    "- **Elbow Method**: Plot inertia vs number of clusters\n",
    "- **Silhouette Analysis**: Plot silhouette scores\n",
    "- **Gap Statistic**: Compare with reference distribution\n",
    "- **Cross-Validation**: Use stability of clusters across folds\n",
    "\n",
    "### 4. Practical Implementation Tips\n",
    "```python\n",
    "# Always scale your data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Use pipelines for clean code\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('cluster', KMeans(n_clusters=3))\n",
    "])\n",
    "\n",
    "# Evaluate multiple algorithms\n",
    "from sklearn.metrics import silhouette_score\n",
    "algorithms = {\n",
    "    'kmeans': KMeans(n_clusters=3),\n",
    "    'dbscan': DBSCAN(eps=0.5),\n",
    "    'spectral': SpectralClustering(n_clusters=3)\n",
    "}\n",
    "\n",
    "for name, algo in algorithms.items():\n",
    "    labels = algo.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels)\n",
    "    print(f\"{name}: {score:.3f}\")\n",
    "```\n",
    "\n",
    "### 5. Debugging Common Issues\n",
    "- **Empty clusters**: Reduce k or change initialization\n",
    "- **Poor separation**: Try different algorithms or preprocessing\n",
    "- **Too many small clusters**: Increase eps (DBSCAN) or reduce k\n",
    "- **Outliers affecting results**: Use robust algorithms or preprocess\n",
    "- **High-dimensional curse**: Apply dimensionality reduction first\n",
    "\n",
    "This comprehensive guide covers all major clustering algorithms and provides the knowledge needed to become proficient in clustering for machine learning engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41e750",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
