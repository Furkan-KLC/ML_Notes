{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd967908",
   "metadata": {},
   "source": [
    "- #  sklearn.preprocessing\n",
    " package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb199272",
   "metadata": {},
   "source": [
    "# Standardization \n",
    "- standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data\n",
    "- The preprocessing module provides the StandardScaler utility class, which is a quick and easy way to perform the following operation on an array-like dataset:\n",
    "- Scaled data has zero mean and unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbef16",
   "metadata": {},
   "source": [
    "# Scaling features to a range:\n",
    "- An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler, respectively.\n",
    "```\n",
    "- X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n",
    "array([[0.5       , 0.        , 1.        ],\n",
    "       [1.        , 0.5       , 0.33333333],\n",
    "       [0.        , 1.        , 0.        ]]) ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eecb05",
   "metadata": {},
   "source": [
    "# Non-linear transformation:\n",
    " - Mapping to a Uniform distribution:\n",
    " - Two types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature.\n",
    " - QuantileTransformer provides a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1:\n",
    " - Mapping to a Gaussian distribution:\n",
    " - n many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd302571",
   "metadata": {},
   "source": [
    "# Normalization:\n",
    "- Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "- The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the l1, l2, or max norms\n",
    "```\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "X_normalized\n",
    "array([[ 0.408, -0.408,  0.812],\n",
    "       [ 1.   ,  0.   ,  0.   ],\n",
    "       [ 0.   ,  0.707, -0.707]]) ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef4d93",
   "metadata": {},
   "source": [
    "# Encoding categorical features: \n",
    "- To convert categorical features to such integer codes, we can use the OrdinalEncoder. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1):\n",
    "```\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "OrdinalEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari']])\n",
    "array([[0., 1., 1.]]) \n",
    "``` \n",
    "\n",
    "- OrdinalEncoder provides a parameter encoded_missing_value to encode the missing values without the need to create a pipeline and using SimpleImputer.\n",
    "- enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n",
    "- nother possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0.\n",
    "```\n",
    "- enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "enc.transform([['female', 'from US', 'uses Safari'],\n",
    "               ['male', 'from Europe', 'uses Safari']]).toarray() \n",
    "\n",
    "```  \n",
    "- TargetEncoder:\n",
    "- The TargetEncoder uses the target mean conditioned on the categorical feature for encoding unordered categories, i.e. nominal categories [PAR] [MIC]. This encoding scheme is useful with categorical features with high cardinality, where one-hot encoding would inflate the feature space making it more expensive for a downstream model to process. A classical example of high cardinality categories are location based such as zip code or region.\n",
    "- Target Encoding must be applied in test set or in cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2c588",
   "metadata": {},
   "source": [
    "# Discretization: \n",
    "- Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes.\n",
    "```\n",
    "X = np.array([[ -3., 5., 15 ],\n",
    "              [  0., 6., 14 ],\n",
    "              [  6., 3., 11 ]])\n",
    "est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)\n",
    "\n",
    "Feature 1 (values: -3, 0, 6; 3 bins)\n",
    "→ intervals: [-3,0), [0,3), [3,6]\n",
    "\n",
    "Feature 2 (values: 3, 5, 6; 2 bins)\n",
    "→ intervals: [3,4.5), [4.5,6]\n",
    "\n",
    "Feature 3 (values: 11, 14, 15; 2 bins)\n",
    "→ intervals: [11,13), [13,15]\n",
    "``` \n",
    "- Feature binarization:\n",
    "- Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the BernoulliRBM.\n",
    "```\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n",
    "binarizer\n",
    "\n",
    "binarizer.transform(X)\n",
    "binarizer = preprocessing.Binarizer(threshold=1.1)  #it is possible to adjust treshold \n",
    "binarizer.transform(X)\n",
    "array([[0., 0., 1.],\n",
    "       [1., 0., 0.],\n",
    "       [0., 0., 0.]])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf81bdc",
   "metadata": {},
   "source": [
    "# Imputation of missing values:\n",
    "- One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (SimpleImputer). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values ( IterativeImputer).\n",
    "-   UNIVARIATE FEATURE IMPUTATION:The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings\n",
    "```\n",
    "- from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "SimpleImputer()\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))\n",
    "[[4.          2.        ]\n",
    " [6.          3.666]\n",
    " [7.          6.        ]]\n",
    " ```\n",
    "- MULTIVARIATE IMPUTATION:\n",
    "- A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n",
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "print(np.round(imp.transform(X_test)))\n",
    "[[ 1.  2.]\n",
    " [ 6. 12.]\n",
    " [ 3.  6.]]\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25737223",
   "metadata": {},
   "source": [
    "# NEAREST NEIGHBORS IMPUTATION:\n",
    "\n",
    "- The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature.\n",
    "\n",
    "\n",
    "```\n",
    "- import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "nan = np.nan\n",
    "X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imputer.fit_transform(X)\n",
    "array([[1. , 2. , 4. ],\n",
    "       [3. , 4. , 3. ],\n",
    "       [5.5, 6. , 5. ],\n",
    "       [8. , 8. , 7. ]])\n",
    "       ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5033aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
