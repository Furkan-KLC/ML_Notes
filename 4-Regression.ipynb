{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0feb7020",
   "metadata": {},
   "source": [
    "REGRESSION :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652aaba",
   "metadata": {},
   "source": [
    "what is regression,Regression is a supervised learning task where the goal is to model the relationship between input features (X) and a continuous target variable (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036aa13",
   "metadata": {},
   "source": [
    "Types of Regression Models:\n",
    "- 1- Linear Regression:\n",
    "- Simple (one feature) or multiple (many features)\n",
    "-  EQUATION  y=β0​+β1​x1​+…+βn​xn <br>\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.linear_model import LinearRegression <br>\n",
    "model=LinearRegression( \n",
    "    fit_intercept=True,                         # whether to calculate intercept β0 <br>\n",
    "    copy_X=True,                                # copy input or not <br>\n",
    "    n_jobs=None,                                # parallel jobs<br>\n",
    "    positive=False                              # force coefficients to be positive<br>\n",
    "    )<br> </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de8c47",
   "metadata": {},
   "source": [
    "2- Polynomial Regression \n",
    "- Extension of linear regression with polynomial terms.\n",
    "- It used for nonlinear relation between independent and dependent variable <br>\n",
    "<span style=\"color:blue\">from sklearn.preprocessing import PolynomialFeatures <br>\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False) <br ></span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4d95c",
   "metadata": {},
   "source": [
    "3. Ridge Regression (L2 Regularization):<br>\n",
    "Ridge regression shrinks coefficients but never sets them exactly to zero. It is useful when features are highly correlated (multicollinearity)<br>\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.linear_model import ridge <br>\n",
    "model = Ridge(\n",
    "    alpha=1.0,\n",
    "    fit_intercept=True,\n",
    "    max_iter=None,\n",
    "    tol=0.001,\n",
    "    solver='auto'\n",
    ") <br>\n",
    "model.fit(X, y) <br>\n",
    "y_pred = model.predict(X)<br>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21222ec",
   "metadata": {},
   "source": [
    "4.LASSO REGRESSION(L1 Regularization):Lasso regression forces some coefficients to become exactly zero, effectively performing feature selection. Best when you expect only a subset of features to be important.<br>\n",
    "<span style=\"color:blue\">\n",
    "model= Lasso(\n",
    "    alpha=0.1,\n",
    "    fit_intercept=True,\n",
    "    max_iter=1000,\n",
    "    tol=0.0001,\n",
    "    selection='cyclic'\n",
    ")<br>\n",
    "model.fit(X, y)<br>\n",
    "y_pred = model.predict(X)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5206c",
   "metadata": {},
   "source": [
    "ElasticNet (L1 + L2):ElasticNet combines Lasso (L1) and Ridge (L2). It balances feature selection and coefficient shrinkage.<br>\n",
    "<span style=\"color:blue\">\n",
    "model = ElasticNet(\n",
    "    alpha=1.0,\n",
    "    l1_ratio=0.5,\n",
    "    fit_intercept=True,\n",
    "    max_iter=1000,\n",
    "    tol=0.0001\n",
    ")<br>\n",
    "model.fit(X, y)<br> \n",
    "y_pred = model.predict(X)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bb903",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINES:\n",
    "-SVR uses the idea of margins (like SVM). Small errors within a margin (ε) are ignored. It can use kernels (linear, polynomial, RBF) to handle nonlinear data.\n",
    "-here are three different implementations of Support Vector Regression: SVR, NuSVR and LinearSVR. LinearSVR also regularizes the intercept<br>\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.svm import SVR <br>\n",
    "model = SVR(\n",
    "    kernel='rbf',    \n",
    "    degree=3,\n",
    "    C=1.0,\n",
    "    epsilon=0.1,\n",
    "    gamma='scale'\n",
    ") other kernels are polynomial,linear,sigmoid<br>      \n",
    "model.fit(X, y)<br>\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d47947a",
   "metadata": {},
   "source": [
    "DECISON TREES:<br>\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. <br>\n",
    "-Decision trees can also be applied to regression problems, using the DecisionTreeRegressor class.<br>\n",
    "-DecisionTreeClassifier is a class capable of performing multi-class classification on a dataset.<br>\n",
    "-Once trained, you can plot the tree with the plot_tree function <br>\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.tree import DecisionTreeRegressor <br>\n",
    "model = DecisionTreeRegressor(\n",
    "    criterion='squared_error',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "<br>\n",
    "model.fit(X, y)<br>\n",
    "y_pred = model.predict(X) <br></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8725a3",
   "metadata": {},
   "source": [
    "RANDOM FOREST REGRESSOR:<br>\n",
    "-The sklearn.ensemble module includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method.\n",
    "- The prediction of the ensemble is given as the averaged prediction of the individual classifiers.\n",
    "- A random forest is a meta estimator that fits a number of decision tree regressors on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy <br>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.ensemble import RandomForestRegressor <br>\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    criterion='squared_error',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")<br>\n",
    "model.fit(X, y)<br>\n",
    "y_pred = model.predict(X) <br></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945bc6be",
   "metadata": {},
   "source": [
    "\n",
    "Gradient Boosting:\n",
    "-builds trees sequentially, each correcting the errors of the previous one. Very powerful, widely used in competitions (XGBoost, LightGBM, CatBoost)<br>\n",
    "-This estimator builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function. <br>\n",
    "<span style=\"color:blue\">from sklearn.ensemble import GradientBoostinRegressor <br>\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")<br>\n",
    "model.fit(X, y)<br>\n",
    "y_pred = model.predict(X)<br></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884510f1",
   "metadata": {},
   "source": [
    "BAYESIAN REGRESSION:\n",
    "-Bayesian regression places distributions over coefficients instead of fixed values. It captures uncertainty in predictions. <br>\n",
    "-To obtain a fully probabilistic model, the output y is assumed to be Gaussian distributed around X <br>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.linear_model import BayesianRidge <br>\n",
    "model = BayesianRidge() <br>\n",
    "model.fit(X, y) <br>\n",
    "y_pred = model.predict(X) </span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc73ce8",
   "metadata": {},
   "source": [
    "Robust Regression (Huber, RANSAC):\n",
    "-Robust regression methods are less sensitive to outliers. Huber mixes squared loss and absolute loss. RANSAC fits models on subsets of data to ignore outliers.<br>\n",
    "-Huber Regressor optimizes the squared loss for the samples where |(y - Xw - c) / sigma| < epsilon and the absolute loss for the samples where |(y - Xw - c) / sigma| > epsilon, where the model coefficients w, the intercept c and the scale sigma are parameters to be optimized. <br>\n",
    "-The Huber loss function has the advantage of not being heavily influenced by the outliers while not completely ignoring their effect.<br>\n",
    "-The RANSAC regressor automatically splits the data into inliers and outliers, and the fitted line is determined only by the identified inliers.<br>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor\n",
    "\n",
    "huber = HuberRegressor()<br>\n",
    "huber.fit(X, y)<br>\n",
    "y_pred_huber = huber.predict(X)<br>\n",
    "\n",
    "ransac = RANSACRegressor() <br>\n",
    "ransac.fit(X, y)<br>\n",
    "y_pred_ransac = ransac.predict(X)<br> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa89d3",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px\">Generalized Linear Models:</span>\n",
    "- Generalized Linear Models (GLM) extend linear models in two ways . First, the predicted values Y are linked to a linear combination of the input X variables via an inverse link function H as :\n",
    "- y^​(w,X)=h(Xw) w is coefficent vector ,X is input matrix, h is inverse link function.\n",
    "-Secondly, the squared loss function is replaced by the unit deviance of a distribution in the exponential family (or more precisely, a reproductive exponential dispersion model (EDM)<br>\n",
    "\n",
    "Normal (Gaussian):\n",
    "Standard linear regression. Assumes errors are normally distributed and variance is constant. Used for predicting any real-valued outcome (e.g., height, temperature).\n",
    "\n",
    "Bernoulli:\n",
    "Binary logistic regression. Used when the target variable has two categories (success/failure, churn/no churn). The link function is the logit.\n",
    "\n",
    "Categorical (Multinomial):\n",
    "Generalization of Bernoulli to multiple categories. Used for multi-class classification problems (predicting discrete labels like animal types, product categories).\n",
    "\n",
    "Poisson:\n",
    "Models counts of events. Assumes variance ≈ mean. Useful for data like number of calls received, number of arrivals, number of accidents.\n",
    "\n",
    "Gamma:\n",
    "Models positive continuous data with skewness. Common in insurance, health economics (e.g., cost, waiting time). Variance grows quadratically with mean.\n",
    "\n",
    "Inverse Gaussian:\n",
    "For positive skewed data with variance that increases even faster than Gamma. Used in survival analysis, reliability engineering, and scenarios where long right tails are expected (e.g., lifetimes of components).\n",
    "\n",
    "TweedieRegressor → A flexible family (can represent normal, Poisson, Gamma, etc., depending on power parameter)\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.linear_model import PoissonRegressor, GammaRegressor, TweedieRegressor <br>\n",
    "\n",
    "poisson = PoissonRegressor(alpha=1.0, max_iter=100) <br>\n",
    "gamma = GammaRegressor(alpha=0.1, max_iter=100)<br>\n",
    "tweedie = TweedieRegressor(power=1.5, alpha=0.1, max_iter=100) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8f7d0",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px\"> Passive Agressive Regressor </span>:<br>\n",
    "-The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However\n",
    "contrary to the Perceptron, they include a regularization parameter C\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.linear_model import PassiveAgressiveRegressor <br>\n",
    "model = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3, C=1.0) </span> <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3215fb",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px\">SGDRegressor (Stochastic Gradient Descent):</span><br>\n",
    "Linear models optimized via stochastic gradient descent.\n",
    "\n",
    "Very efficient for huge datasets.\n",
    "\n",
    "Can be combined with different penalties (l1, l2, elasticnet).<br>\n",
    "<span style=\"color:blue\"><br>\n",
    "from sklearn.linear_model import SGDRegressor <br>\n",
    "model = SGDRegressor(loss='squared_error', penalty='l2', alpha=0.0001, max_iter=1000)<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82a39e",
   "metadata": {},
   "source": [
    "<span style=\"font-szie:20px\">MULTI-TASK REGRESSORS:</span><br>\n",
    "-The MultiTaskLasso is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.linear_model import MultiTaskLasso, MultiTaskElasticNet\n",
    "\n",
    "mtlasso = MultiTaskLasso(alpha=0.1)\n",
    "mtelastic = MultiTaskElasticNet(alpha=0.1, l1_ratio=0.5) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770141bd",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; font-weight:600;\">Isotonic Regression</span>\n",
    "\n",
    "Non-parametric regression that fits a free-form line constrained to be monotonic.\n",
    "\n",
    "✅ Useful when we know the relationship is always increasing or decreasing.\n",
    "⚠️ Limitation: works only for 1D features.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(X, y)\n",
    "y_pred = iso.predict(X_new)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e9715",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; font-weight:600;\">k-Nearest Neighbors Regression</span>\n",
    "\n",
    "Predicts the target as the average of k nearest neighbors in the feature space.\n",
    "\n",
    "✅ Flexible, non-parametric, good for multi-output.\n",
    "⚠️ Sensitive to scaling and slow for large datasets.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(\n",
    "n_neighbors=5,\n",
    "weights='distance'\n",
    ")\n",
    "knn.fit(X, y)\n",
    "y_pred = knn.predict(X_new)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50203a3b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; font-weight:600;\">Gaussian Process Regression</span>\n",
    "\n",
    "A Bayesian, non-parametric regression that places a prior over functions, updated with observed data.\n",
    "\n",
    "✅ Produces not just predictions but uncertainty estimates.\n",
    "⚠️ Computationally expensive for large datasets (O(n³)).\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor <br>\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5)\n",
    "\n",
    "gpr.fit(X, y)\n",
    "y_mean, y_std = gpr.predict(X_new, return_std=True)\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
