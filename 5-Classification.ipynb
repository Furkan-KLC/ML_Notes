{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46077a33",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30px\">Classification</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6abfa6",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION:\n",
    "- Logistic Regression (also called Logit Regression) is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled “1”), or else it predicts that it does not (i.e., it belongs to the negative class, labeled “0”). This makes it a binary classifier. <br>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.linear_model import LogisticRegression<br>\n",
    "log_reg = LogisticRegression() <br>\n",
    "log_reg.fit(X, y)<br>\n",
    " </span> <BR>\n",
    " - WHAT IF WE WANT TO CLASSIFY MORE THAN 2 CATEGORIES WITH LOGISTIC REGRESSION ? --> WE USE \"one-vs-rest or multinomial methods\" <br>\n",
    " <span style=\"color:blue\">\n",
    " -  Load data <br>\n",
    "- iris = datasets.load_iris() <br>\n",
    "- features = iris.data target = iris.target<br>\n",
    "- Standardize features<br>\n",
    "- scaler = StandardScaler() features_standardized = scaler.fit_transform(features)<br>\n",
    "-Create one-vs-rest logistic regression object  <br>\n",
    "- logistic_regression = LogisticRegression(random_state=0, multi_class=\"ovr\")<br>\n",
    "- Train model <br>\n",
    "- model = logistic_regression.fit(features_standardized, target) <br>\n",
    "</span>\n",
    "\n",
    "- WHAT IF WE NEED TO REDUCE VARIANCE OF OUR LOGISTIC REGRESSION MODEL ? --> Tune the regularization strength hyperparameter, C:\n",
    "- logistic_regression = LogisticRegressionCV( penalty='l2', Cs=10, random_state=0, n_jobs=-1) #WE USED LogisticRegressionCV() cs is params for C VALUES.\n",
    "\n",
    "- WHAT IF WE NEED TO CLASSIFY VERY LARGE DATA WITH LOGISTIC REGRESSION ? --> CHANGE SOLVER TO \"sag\" \n",
    "<br>\n",
    "- <span style=\"color:blue\">logistic_regression = LogisticRegression(random_state=0, solver=\"sag\") </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddfec0",
   "metadata": {},
   "source": [
    "# STOCHASTIC GRADIENT DESCENT:\n",
    "- Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time\n",
    "\n",
    "- This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.<br>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "-from sklearn.linear_model import SGDClassifier()<br>\n",
    "- recommended parameter  values for start :\n",
    "- Learning rate: 0.01 (SGD) or 0.001 (with momentum/Adam).\n",
    "\n",
    "    Batch size: 32–128.\n",
    "\n",
    "    Momentum: 0.9.\n",
    "\n",
    "    Weight decay: 1e-4.\n",
    " </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb26f87",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINES:\n",
    "- Support vector machines classify data by finding the hyperplane that maximizes the margin between the classes in the training data. In a two-dimensional example with two classes, we can think of a hyperplane as the widest straight “band” that separates the two classes.\n",
    "- SVC, NuSVC and LinearSVC are classes capable of performing binary and multi-class classification on a dataset.\n",
    "- SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations  On the other hand, LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel\n",
    "\n",
    "- WHAT IF WE HAVE LINEARLY NOT SEPERABLE DATA HOW WE CAN CLASSIFY IT ?\n",
    "- we can change kernel parameters:\n",
    "- Polynomial kernel – good for data with polynomial relations.\n",
    "- Radial Basis Function (RBF) kernel – handles complex, nonlinear boundaries.\n",
    "- Sigmoid kernel – sometimes used in neural-net–like scenarios.\n",
    "  \n",
    "-The C parameter controls the penalty for misclassification:\n",
    "- High C → less tolerance for errors (harder margin).\n",
    "- Low C → more tolerance, allows some misclassified points, better generalization.\n",
    "- Feature Engineering / Transformation,and Data preprocessing. <br>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.svm import SVC <br>\n",
    "svc = SVC(kernel=\"rbf\", random_state=0, gamma=1, C=1) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee65a1b",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier:\n",
    "- Decision tree learners attempt to find a decision rule that produces the greatest decrease in impurity at a node. While there are a number of measurements of impurity, by default DecisionTreeClassifier uses Gini impurity\n",
    "- Gini(t) = 1 - Σ (p_i^2),  i = 1 to C \n",
    "-where G(t) is the Gini impurity at node t and pi is the proportion of observations of class c at node t. This process of finding the decision rules that create splits to increase impurity is repeated recursively until all leaf nodes are pure ... <br>\n",
    "<span style=\"color:blue\">\n",
    " from sklearn.tree import DecisionTreeClassifier <br >\n",
    " clf = DecisionTreeClassifier(random_state=0)\n",
    "   </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ecdd5a",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASSIFIER:\n",
    "- This algorithm builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced\n",
    "- HOW IT WORKS:\n",
    "- Generate B bootstrap samples\n",
    "- For each sample, build decision tree with random feature selection\n",
    "- Train all trees independently\n",
    "- For prediction, pass input through all trees\n",
    "- Aggregate predictions using majority vote\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "sklearn.ensemble.RandomForestClassifier()  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed1712",
   "metadata": {},
   "source": [
    "# HISTGRADIENTBOOSTINGCLASSIFIER:\n",
    "- This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).\n",
    "    This estimator has native support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently.\n",
    "\n",
    "<span style=\"color:blue\">  \n",
    "sklearn.ensemble.HistGradientBoostingClassifier() \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee34ad",
   "metadata": {},
   "source": [
    "# K-NEIGHBORS CLASSIFIER :\n",
    "- Step-1: Select the number K of the neighbors\n",
    "-  Step-2: Calculate the Euclidean,Manhattan or minkowski distance of K number of neighbors\n",
    "-  Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n",
    "-  Step-4: Among these k neighbors, count the number of the data points in each category.\n",
    "-  Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n",
    "-  Step-6: Our model is ready.\n",
    " \n",
    " <span style=\"color:blue\">\n",
    " from sklearn.neighbors import NearestNeighbors </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758f469",
   "metadata": {},
   "source": [
    "# NAIVE BAYES CLASSIFIER:\n",
    "\n",
    "- How it works:\n",
    "- Bayes' Theorem: Apply P(A|B) = P(B|A) × P(A) / P(B)\n",
    "- Independence Assumption: Assume features are conditionally independent\n",
    "- Prior Probability: Calculate prior probability for each class\n",
    "- Likelihood: Calculate likelihood of features given each class\n",
    "- Posterior: Calculate posterior probability for each class\n",
    "- Classification: Choose class with highest posterior probability\n",
    "\n",
    "The most common type of naive Bayes classifier is the Gaussian naive Bayes. In Gaussian naive Bayes, we assume that the likelihood of the feature values, x, given an observation is of class y, follows a normal distribution\n",
    "\n",
    "- WHEN WE HAVE discrete or count data, you need to train a naive Bayes classifier, WE USE MULTINOMIAL NAIVE BAYES CLASSIFIER\n",
    "- You have binary feature data and need to train a naive Bayes classifier. WE USE BERNOULLI NAIVE BAYES CLASSIFIER\n",
    "- You want to calibrate the predicted probabilities from naive Bayes classifiers so they are interpretable. USE  CalibratedClassifierCV <br>\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.naive_bayes import GaussianNB <br>\n",
    "from sklearn.naive_bayes import MultinomialNB <br> \n",
    "from sklearn.calibration import CalibratedClassifierCV </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e483397",
   "metadata": {},
   "source": [
    "# LinearDiscriminantAnalysis - QuadraticDiscriminantAnalysis\n",
    "- LinearDiscriminantAnalysis  and QuadraticDiscriminantAnalysis are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, \n",
    " respectively.\n",
    "- These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and \n",
    " have no hyperparameters to tune\n",
    "- How It Works: LDA performs classification by finding linear boundaries between classes. It uses Bayes' theorem and assumes Gaussian distribution.\n",
    "- Step-by-Step Working Process:\n",
    "- Calculate the mean for each class\n",
    "- Compute the common covariance matrix for all data\n",
    "- Calculate prior probabilities for each class\n",
    "- For new data points, compute discriminant function value for each class\n",
    "- Assign to the class with the highest discriminant value \n",
    "- Quadratic Discriminant Analysis (QDA)\n",
    "- How It Works:QDA is a generalized version of LDA. It uses separate covariance matrices for each class and creates quadratic decision boundaries.\n",
    "- Calculate the mean for each class\n",
    "- Compute separate covariance matrix for each class\n",
    "- Calculate prior probabilities for each class\n",
    "- For new data points, compute quadratic discriminant function value for each class\n",
    "- Assign to the class with the highest discriminant value<br>\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis<br>\n",
    "LDA <br>\n",
    "lda = LinearDiscriminantAnalysis()<br>\n",
    "QDA   <br>\n",
    "qda = QuadraticDiscriminantAnalysis() <br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1e7ed",
   "metadata": {},
   "source": [
    "# BAGGING CLASSIFIER:\n",
    "- In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and\n",
    " then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator.\n",
    "-  Step-by-step process:\n",
    "- Generate B bootstrap samples from training data (typically B=10-100)\n",
    "- Train a base classifier on each bootstrap sample\n",
    "- For feature bagging, randomly select subset of features for each model\n",
    "- Store all trained models\n",
    "- For new predictions, get prediction from each model\n",
    "- Combine predictions using majority voting (classification) or averaging (regression)\n",
    "- Return final ensemble prediction <br>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "from sklearn.ensemble import BaggingClassifier </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7c1ce",
   "metadata": {},
   "source": [
    "# AdaBoostClassifier:\n",
    "- AdaBoost (Adaptive Boosting) sequentially trains weak learners, where each subsequent model focuses on correcting the mistakes of previous models by giving higher weights to misclassified examples.\n",
    "- Step-by-step process:\n",
    "- Initialize equal weights for all training examples\n",
    "- Train first weak learner on weighted dataset\n",
    "- Calculate error rate of current model\n",
    "- Calculate model weight based on error rate (lower error = higher weight)\n",
    "- Update example weights (increase for misclassified, decrease for correct)\n",
    "- Normalize weights so they sum to 1\n",
    "- Repeat steps 2-6 for specified number of iterations\n",
    "- Final prediction = weighted vote of all weak learners <br>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "-from sklearn.ensemble import AdaBoostClassifier <br>\n",
    "-from sklearn.tree import DecisionTreeClassifier <br>\n",
    "\n",
    "-adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")   </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f9fdc",
   "metadata": {},
   "source": [
    "# VotingClassifier\n",
    "- How it works:VotingClassifier combines predictions from multiple different algorithms using either majority voting (hard voting) or averaging predicted probabilities (soft voting).\n",
    "- Step-by-step process:\n",
    "- Train each base model independently on the same training data\n",
    "- Store all trained models\n",
    "- For new predictions, get prediction from each model\n",
    "- Hard Voting: Count votes for each class, assign majority class\n",
    "- Soft Voting: Average predicted probabilities, assign class with highest average probability\n",
    "- Return ensemble prediction\n",
    "```\n",
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('svc', SVC(probability=True)),  # probability=True for soft voting\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ],\n",
    "    voting='soft'  # or 'hard'\n",
    ")\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed6299",
   "metadata": {},
   "source": [
    "# StackingClassifier\n",
    "-How it works:StackingClassifier uses a meta-model (meta-learner) to learn how to best combine predictions from multiple base models. Base models make predictions, which become features for the meta-model.\n",
    "- Step-by-step process:\n",
    "- Level-0 Training: Train base models using cross-validation\n",
    "\n",
    "- Split training data into K folds\n",
    "    For each fold, train base models on other K-1 folds\n",
    "    Predict on held-out fold\n",
    "    Combine predictions to create meta-features\n",
    "\n",
    "- Meta-Feature Creation: \n",
    "    Base model predictions become new feature matrix\n",
    "    Level-1 Training: Train meta-model on meta-features and original targets\n",
    "    Final Model: Store both base models and meta-model\n",
    "- Prediction:\n",
    "    Get predictions from all base models\n",
    "    Use these predictions as input to meta-model\n",
    "    Return meta-model prediction\n",
    "```\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('svc', SVC()),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5  # Number of cross-validation folds\n",
    ") ``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c091b4a",
   "metadata": {},
   "source": [
    "   | Method     | Training        | Combination     | Complexity | Best For                 |\n",
    "|------------|-----------------|-----------------|------------|--------------------------|\n",
    "| Bagging    | Parallel        | Majority Vote   | Low        | High-variance models     |\n",
    "| AdaBoost   | Sequential      | Weighted Vote   | Medium     | Weak learners            |\n",
    "| Voting     | Parallel        | Vote/Average    | Low        | Diverse good models      |\n",
    "| Stacking   | CV-based        | Meta-learning   | High       | Maximum performance      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e7bd2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
